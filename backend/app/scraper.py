import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict
import os
from dotenv import load_dotenv
from sqlalchemy.orm import Session
from app.models import Review, SystemLog
import time

load_dotenv()

class ReviewScraper:
    def __init__(self, base_url: str = None):
        self.base_url = base_url or os.getenv("DUMMY_SITE_URL", "http://localhost:5000")
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def scrape_reviews(self) -> List[Dict]:
        """ë”ë¯¸ ì‚¬ì´íŠ¸ì—ì„œ ë¦¬ë·°ë¥¼ ìŠ¤í¬ë˜í•‘ (API ë°©ì‹)"""
        try:
            # ë”ë¯¸ ì‚¬ì´íŠ¸ëŠ” JSON APIë¥¼ ì œê³µí•˜ë¯€ë¡œ ì§ì ‘ APIë¥¼ í˜¸ì¶œ
            api_url = f"{self.base_url}/api/reviews"
            print(f"ğŸ” {api_url} ì—ì„œ ë¦¬ë·° ìˆ˜ì§‘ ì¤‘...")
            
            response = self.session.get(api_url, timeout=10)
            response.raise_for_status()
            
            # JSON ë°ì´í„° íŒŒì‹±
            reviews_data = response.json()
            reviews = []
            
            for review in reviews_data:
                try:
                    # ë‚ ì§œ íŒŒì‹± (ë”ë¯¸ ì‚¬ì´íŠ¸ëŠ” 'YYYY-MM-DD' í˜•ì‹)
                    review_date_str = review.get('date', '')
                    # ì‹œê°„ì´ ì—†ìœ¼ë©´ 00:00:00 ì¶”ê°€
                    if len(review_date_str) == 10:  # YYYY-MM-DD
                        review_date_str += ' 00:00:00'
                    
                    review_date = datetime.strptime(review_date_str, '%Y-%m-%d %H:%M:%S')
                    
                    reviews.append({
                        'source_id': f"dummy_{review['id']}",
                        'customer_name': review.get('customer_name', 'ìµëª…'),
                        'review_text': review.get('review_text', ''),
                        'review_date': review_date
                    })
                    
                except Exception as e:
                    print(f"âš ï¸  ê°œë³„ ë¦¬ë·° íŒŒì‹± ì—ëŸ¬: {e}")
                    print(f"    ë¬¸ì œ ë¦¬ë·° ë°ì´í„°: {review}")
                    continue
            
            print(f"âœ“ {len(reviews)}ê°œì˜ ë¦¬ë·°ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return reviews
            
        except requests.RequestException as e:
            print(f"âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    def save_reviews_to_db(self, db: Session, reviews: List[Dict]) -> int:
        """ìˆ˜ì§‘í•œ ë¦¬ë·°ë¥¼ DBì— ì €ì¥ (ì¤‘ë³µ ì²´í¬)"""
        saved_count = 0
        
        for review_data in reviews:
            try:
                # ì¤‘ë³µ ì²´í¬
                existing = db.query(Review).filter(
                    Review.source_id == review_data['source_id']
                ).first()
                
                if existing:
                    print(f"â­ï¸  ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¦¬ë·°: {review_data['source_id']}")
                    continue
                
                # ìƒˆ ë¦¬ë·° ì €ì¥
                new_review = Review(**review_data)
                db.add(new_review)
                db.commit()
                saved_count += 1
                print(f"âœ“ ìƒˆ ë¦¬ë·° ì €ì¥: {review_data['customer_name']} - {review_data['source_id']}")
                
            except Exception as e:
                db.rollback()
                print(f"âŒ ë¦¬ë·° ì €ì¥ ì‹¤íŒ¨: {e}")
                
                # ì—ëŸ¬ ë¡œê·¸ ì €ì¥
                log = SystemLog(
                    log_type="error",
                    message=f"ë¦¬ë·° ì €ì¥ ì‹¤íŒ¨: {review_data.get('source_id')}",
                    details={"error": str(e), "review_data": review_data}
                )
                db.add(log)
                db.commit()
        
        return saved_count
    
    def run_scraping_job(self, db: Session):
        """ìŠ¤í¬ë˜í•‘ ì‘ì—… ì‹¤í–‰ (ë¡œê¹… í¬í•¨)"""
        start_time = time.time()
        
        # ì‹œì‘ ë¡œê·¸
        log = SystemLog(
            log_type="scraping",
            message="ìŠ¤í¬ë˜í•‘ ì‘ì—… ì‹œì‘",
            details={"url": self.base_url}
        )
        db.add(log)
        db.commit()
        
        try:
            reviews = self.scrape_reviews()
            saved_count = self.save_reviews_to_db(db, reviews)
            
            elapsed = round(time.time() - start_time, 2)
            
            # ì™„ë£Œ ë¡œê·¸
            log = SystemLog(
                log_type="scraping",
                message=f"ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {saved_count}ê°œ ì €ì¥",
                details={
                    "total_scraped": len(reviews),
                    "saved_count": saved_count,
                    "elapsed_seconds": elapsed
                }
            )
            db.add(log)
            db.commit()
            
            print(f"\nğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼:")
            print(f"   - ìˆ˜ì§‘: {len(reviews)}ê°œ")
            print(f"   - ì €ì¥: {saved_count}ê°œ")
            print(f"   - ì†Œìš”ì‹œê°„: {elapsed}ì´ˆ")
            
            return saved_count
            
        except Exception as e:
            # ì—ëŸ¬ ë¡œê·¸
            log = SystemLog(
                log_type="error",
                message=f"ìŠ¤í¬ë˜í•‘ ì‘ì—… ì‹¤íŒ¨: {str(e)}",
                details={"error": str(e)}
            )
            db.add(log)
            db.commit()
            raise


# í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    from database import SessionLocal
    
    scraper = ReviewScraper()
    db = SessionLocal()
    
    try:
        scraper.run_scraping_job(db)
    finally:
        db.close()